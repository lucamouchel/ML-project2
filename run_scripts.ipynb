{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.18\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: This is a good tweet.\n",
      "positive: 0.7955\n",
      "negative: 0.2045\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "tweet = \"This is a good tweet.\"\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "result = classifier(tweet, candidate_labels)\n",
    "print(f\"Tweet: {tweet}\")\n",
    "for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "    print(f\"{label}: {score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: nltk in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "255it [00:00, 11468.94it/s]\n",
      "64it [00:00, 17098.89it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "12/20/2023 16:56:47 - INFO - project.src.testingScripts.Trainer_test -   ***** Running training *****\n",
      "12/20/2023 16:56:47 - INFO - project.src.testingScripts.Trainer_test -     Num examples = 255\n",
      "12/20/2023 16:56:47 - INFO - project.src.testingScripts.Trainer_test -     Num Epochs = 1\n",
      "12/20/2023 16:56:47 - INFO - project.src.testingScripts.Trainer_test -     Instantaneous batch size per GPU = 32\n",
      "12/20/2023 16:56:47 - INFO - project.src.testingScripts.Trainer_test -     Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "12/20/2023 16:56:47 - INFO - project.src.testingScripts.Trainer_test -     Gradient Accumulation steps = 4\n",
      "12/20/2023 16:56:47 - INFO - project.src.testingScripts.Trainer_test -     Total optimization steps = 2\n",
      "Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[Atorch.Size([32, 52])\n",
      "torch.Size([52])\n",
      "********\n",
      "torch.Size([32])\n",
      "\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:12,  1.80s/it]\u001b[Atorch.Size([32, 52])\n",
      "torch.Size([52])\n",
      "********\n",
      "torch.Size([32])\n",
      "\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:03<00:10,  1.74s/it]\u001b[Atorch.Size([32, 52])\n",
      "torch.Size([52])\n",
      "********\n",
      "torch.Size([32])\n",
      "\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:05<00:08,  1.67s/it]\u001b[Atorch.Size([32, 52])\n",
      "torch.Size([52])\n",
      "********\n",
      "torch.Size([32])\n",
      "12/20/2023 16:56:54 - INFO - project.src.testingScripts.Trainer_test -     Num examples = 64\n",
      "12/20/2023 16:56:54 - INFO - project.src.testingScripts.Trainer_test -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  50%|████████████████▌                | 1/2 [00:00<00:00,  2.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  2.27it/s]\u001b[A\u001b[A\n",
      "accuracy improved, previous 0, new one 0.53125\n",
      "f1 improved, previous 0, new one 0.3992490613266583\n",
      "\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:08<00:09,  2.44s/it]\u001b[Atorch.Size([32, 52])\n",
      "torch.Size([52])\n",
      "********\n",
      "torch.Size([32])\n",
      "\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:10<00:06,  2.08s/it]\u001b[Atorch.Size([32, 52])\n",
      "torch.Size([52])\n",
      "********\n",
      "torch.Size([32])\n",
      "\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:11<00:03,  1.90s/it]\u001b[Atorch.Size([32, 52])\n",
      "torch.Size([52])\n",
      "********\n",
      "torch.Size([32])\n",
      "\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:13<00:01,  1.80s/it]\u001b[Atorch.Size([31, 52])\n",
      "torch.Size([52])\n",
      "********\n",
      "torch.Size([31])\n",
      "12/20/2023 16:57:02 - INFO - project.src.testingScripts.Trainer_test -     Num examples = 64\n",
      "12/20/2023 16:57:02 - INFO - project.src.testingScripts.Trainer_test -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  50%|████████████████▌                | 1/2 [00:00<00:00,  2.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|█████████████████████████████████| 2/2 [00:00<00:00,  2.15it/s]\u001b[A\u001b[A\n",
      "accuracy improved, previous 0.53125, new one 0.71875\n",
      "f1 improved, previous 0.3992490613266583, new one 0.7085020242914979\n",
      "\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:16<00:00,  2.06s/it]\u001b[A\n",
      "Epoch: 100%|██████████████████████████████████████| 1/1 [00:16<00:00, 16.51s/it]\n"
     ]
    }
   ],
   "source": [
    "!python3.8 project/src/testingScripts/train_test.py --language-model nlptown/bert-base-multilingual-uncased-sentiment --epochs 1 --batch-size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: scikit-learn in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "nlptown/bert-base-multilingual-uncased-sentiment\n",
      "80it [00:00, 6858.62it/s]\n",
      "12/20/2023 16:57:12 - INFO - project.src.testingScripts.Trainer_test -     Num examples = 80\n",
      "12/20/2023 16:57:12 - INFO - project.src.testingScripts.Trainer_test -     Batch size = 16\n",
      "Evaluating: 100%|█████████████████████████████████| 5/5 [00:01<00:00,  4.53it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3.8 project/src/testingScripts/predict_test.py --model-dir testing_models/nlptown_bert-base-multilingual-uncased-sentiment --per_gpu_eval_batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

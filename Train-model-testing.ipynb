{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.17\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from transformers==4.17) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from transformers==4.17) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from transformers==4.17) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from transformers==4.17) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from transformers==4.17) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from transformers==4.17) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from transformers==4.17) (2.31.0)\n",
      "Collecting sacremoses (from transformers==4.17)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from transformers==4.17) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from transformers==4.17) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from requests->transformers==4.17) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from requests->transformers==4.17) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from requests->transformers==4.17) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from requests->transformers==4.17) (2023.11.17)\n",
      "Requirement already satisfied: click in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from sacremoses->transformers==4.17) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages (from sacremoses->transformers==4.17) (1.2.0)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sacremoses, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.32.1\n",
      "    Uninstalling transformers-4.32.1:\n",
      "      Successfully uninstalled transformers-4.32.1\n",
      "Successfully installed sacremoses-0.1.1 transformers-4.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, Trainer, TrainingArguments, ElectraTokenizer, ElectraForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'vinai/bertweet-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device=device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            do_eval=True,\n",
    "            do_train=True,\n",
    "            num_train_epochs=4,\n",
    "            save_total_limit=4,\n",
    "            load_best_model_at_end=True,\n",
    "            learning_rate=1e-04,\n",
    "            per_device_train_batch_size=64,\n",
    "            per_device_eval_batch_size=64,\n",
    "            save_strategy=\"steps\",\n",
    "            logging_strategy=\"steps\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            logging_steps=1000,\n",
    "            eval_steps=1000,\n",
    "            save_steps=1000,\n",
    "            optim=\"adamw_hf\",\n",
    "        )\n",
    "\n",
    "def compute_metrics(pred):\n",
    "            labels = pred.label_ids\n",
    "            preds = pred.predictions.argmax(-1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                labels, preds, average=\"weighted\"\n",
    "            )\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv('data/train_pos.txt', delimiter='\\t', header=None).sample(11000, random_state=4)\n",
    "pos['label'] = 1\n",
    "neg = pd.read_csv('data/train_neg.txt', delimiter='\\t', header=None).sample(11000, random_state=4)\n",
    "neg['label'] = -1\n",
    "\n",
    "train_test_split = 0.8\n",
    "\n",
    "full_df = pd.concat([pos, neg]).sample(frac=1, random_state=42)\n",
    "train_df = full_df[:int(len(full_df)*train_test_split)]\n",
    "dev_df = train_df.sample(frac=0.2, random_state=42).rename(columns={0: 'tweet'})\n",
    "train_df = train_df.drop(dev_df.index).rename(columns={0: 'tweet'})\n",
    "test_df = full_df[int(len(full_df)*train_test_split):].rename(columns={0: 'tweet'})\n",
    "# test_df = pd.read_csv('data/test_data.txt', delimiter='\\t', header=None).rename(columns={0: 'tweet'})\n",
    "validation_df = test_df.copy()\n",
    "test_df['label'] = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df['label'])\n",
    "train_df['label'] = label_encoder.transform(train_df['label'])\n",
    "dev_df['label'] = label_encoder.transform(dev_df['label'])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(dev_df),\n",
    "    #'test': Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948aa2570b3f4ee1b69caf12e45a4862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315546b732464317acab7eb7bdd642d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process(batch):\n",
    "    inputs = tokenizer(batch[\"tweet\"], truncation=True, padding=\"max_length\")\n",
    "    return {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"labels\": batch[\"label\"],\n",
    "        }\n",
    "    \n",
    "tokenized_dataset = dataset.map(process, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colinsmyth/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae9d8e542f84ae69d4da456cb2aac4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m             args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m             tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      8\u001b[0m         )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML_PROJ2/lib/python3.8/site-packages/transformers/trainer.py:1839\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1837\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1840\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics,\n",
    "            train_dataset=tokenized_dataset[\"train\"],\n",
    "            eval_dataset=tokenized_dataset[\"validation\"],\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "trainer.train()\n",
    "#trainer.save_model(\"models/electra_classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(text, classes):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    return -1 if predicted_class == 0 else 1\n",
    "\n",
    "inference(\"you are a good person\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
